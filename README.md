Это основной репозиторий оркестратора ETL-процессов. Здесь храняться ДАГи всех проектов, общие функциональные модули и в некоторых случаях sql-скрипты. Репозиторий синхронизирован с тестовым и продуктивным сервером(ветки master & production соответственно).

## Table of contents

- [Documentation](#documentation)
- [Requirements](#requirements)
- [Development environment](#development-environment)
- [Description](#description)
- [Workflow](#workflow)
- [Data](#data)
- [Support](#support)
- [Roadmap](#roadmap)
- [Authors and acknowledgment](#authors-and-acknowledgment)

## Documentation
- [Технические особенности фреймфорка](docs/tech.md)

## Requirements
На текущий момент разработка ведется на версии `Airflow 2.10.3` и `Python 3.11`

В файле [requirements.txt](requirements.txt) находятся рекомендуемые pip-пакеты.

## Development environment

Окружение разработки (база данных PostgreSQL и сервисы Airflow) можно поднять с помощью [docker-compose.yml](.devcontainer/docker-compose.yml)

### Использование преднастройки для VS Code:

0. для следующего шага у вас должны быть установлены: docker, git. для винды Docker Desktop нужно запустить
1. устанавливаем [VS Code](https://code.visualstudio.com/) и расширение `Dev Containers` (ставится из приложения)
1. клонируем репозиторий с использованием персонального токена:
    - получаем токен в [профиле git-lab](https://git.element-lab.ru/-/profile/personal_access_tokens)
    - выполняем в желаемом месте на своём компьютере:\
`git clone https://oauth2:<ваш_токен>@git.element-lab.ru/oms-management-services/etl/airflow.git`
1. открываем склонированный репозиторий в VS Code
1. в окне VS Code открываем терминал и выполняем:\
`git config --get-regexp "user\.(name|email)" > .devcontainer/cfg/git_host.tmp`
1. запускаем сборку контейнера: в окне VS Code нажимаем `F1` и выбираем `Reopen in container`
1. после успешного завершения предыдущей комманды веб-интерфейс Airflow можно найти на [localhost:8080](http://localhost:8080/) (изменить порт веб-сервера можно в файле [docker-compose.yml](.devcontainer/docker-compose.yml))
1. настройка подключения к БД: на главной страничке переходим в Admin -> Connections, нажимаем `[+]` и указываем:
    > Connection Type: `Postgres`\
    > Connection Id: `nds_dds` для работы со слоем ддс, `nds_dma` для работы со слоем витрин\
    > Host / Login / Password / Port - получаем у ответственных
1. для подключения репозиториев [SQL](https://git.element-lab.ru/oms-management-services/etl/sql) и [Data Quality](https://git.element-lab.ru/oms-management-services/etl/data-quality) - выполнить в терминале контейнера:\
`python .devcontainer/setup_scripts/ext_repos/auto.py`

## Description
Процесс добавления нового дага состоит из совместной работы ETL-инженера и аналитика данных. Инженер реализует логику процесса на Python (последовательность выполнения задач, расписание, логирование, отладка), аналитик разрабатывает SQL-запросы, выполняемые в базе данных.
При работе с Python используются все возможности TaskFlow API – современного подхода к разработке кода для Airflow. Целью является создание гибкого и модульного дага, простого в отладке и доработке.

## Workflow
Процесс создания нового ДАГа:
- [ ] Как говорилось ранее создание ETL это совместная работа аналитиков и инженеров. Аналитик определяет задачу, масштаб нового процесса, примеры взаимодействия с базой данных. На осоновании полученных вводных инженер формирует новый ДАГ и шаблон sql-скриптов, по которому аналитики будут создавать свои потоки данных. Цель: ограничить работу аналитиков исключительно базой данных и при этом дать им возможность передавать логику последовательности запуска их скриптов с помощью форматов определенных шаблонов([Подробнее](https://wiki.element-lab.ru/pages/viewpage.action?pageId=34277044)).
- [ ] В разработке ДАГа по максимуму используются уже имеющиеся в airflow модули логирования, рендеринга sql-файлов, соединения с базой и тд. Это облегчает разработку, уменьшает количество кода и открывает некоторые возможности интеграции с интерфейсом самой программы. Рекомендации по работе с кодом берутся из официальной документации [airflow](https://airflow.apache.org/docs/apache-airflow/2.4.1/).
- [ ] SQL-скрипты попадают в репозиторий после утверждения их старшим аналитиком. В шаблонах sql используются переменные формата jinja. Переменные определяются необходимыми в скрипте фильтрами и другими изменяющимися данными. Шаблоны названия самих файлов крайне важны и позволяют аналитикам задавать через них последовательность запуска скриптов без каких-либо дополнительных взаимодействий с оркестратором, также и убирает с etl-инженера необходимость думать об этом. Пример: все скрипты нового процесса содержат некоторое количество подготовительных шагов, очистку дублирующихся данных и вставку данных в целевую таблицу в конце. Исходя из этой информации даг принимает файлы типа {название целевой таблицы}_{название действия}_{номер шага по необходимости}. Например: table_stage_1.sql, table_stage_2.sql, table_insert, table_delete. Подробнее в статье вики [Подход к работе с sql](https://wiki.element-lab.ru/pages/viewpage.action?pageId=34277044).
- [ ] Задача ETL-инженера сделать даг таким образом, чтобы он принимал на вход все скрипты, разрабатываемые в текущем проекте, пока они соответсвуют принятым в рамках него форматам.
- [ ] Во всех дагах используются единые модули(функции) для работы с sql-скриптам, логирования процессов в базу данных и проверке качества данных.

Технически рекомендуется брать за основу существующие даги(dds_etl_dictionary,dds_etl_scheta) или даг-пример example_dag. Он представляет из себя скелет типичного потока данных, обогащенный комментариями.
Обязательными для каждого дага являются шаги:
- [ ] Инициализации - создания ид процесса elt_run, ид дага etl_proc и ид процесса таблицы etl_proc_table в схеме meta.
- [ ] Финализации - закрытие логов процессов.
- [ ] Функция ошибки - функция, отрабатывающая при прерывании работы дага по любой причине. Закрывает все связанные логи со статусом и текстом ошибки, выполняет необходимые дополнительные действия.
Для шага работы с данными также разработан набор универсальных функций, но он не является обязательным. В даге-примере это prepare_task.

## Data
Работа с данными на данный момент заключается в процессе получения сырых медицинских данных, последующей их нормализации и формировании отчетных витрин на основании этих нормализованных данных.
Краткое описание схем в БД:
- ods_nsi - схема всех необходимых справочников
- ods_{номер региона} - медицинские данные конкретного региона(все схемы имеют идентичную архитектуру)
- dds - нормализованные данные, как справочники так и основные мед. данные
- stage - схема для промежуточных(временных) таблиц ETL-процессов
- meta - схема метаданных ETL, логов процессов и результатов проверок
- dma - схема витрин.
